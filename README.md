# CompliAI - Policy & Compliance Assistant for Companies

A streamlined Generative AI solution that helps employees understand internal company policies through natural-language questions using RAG (Retrieval-Augmented Generation) architecture.

## ğŸ¯ Project Overview

### Real-World Problem

Employees across organizations struggle with:

- **Information Overload**: Policy documents spanning hundreds of pages
- **Ambiguous Language**: Complex legal and technical terminology
- **Time-Consuming Search**: Hours spent finding specific policy answers
- **Inconsistent Interpretation**: Different employees interpret policies differently
- **HR/Legal Bottleneck**: 40-60% of support time spent on repetitive questions

### Why Generative AI + RAG?

- **Semantic Understanding**: Retrieves contextually relevant sections beyond keyword matching
- **Natural Language Interface**: Conversational question answering
- **Grounded Responses**: Answers anchored in actual policy documents
- **Scalability**: Handles thousands of queries without human intervention
- **Citation Transparency**: Provides source references for verification

### Expected Impact

- âœ… 70% faster policy lookup time
- âœ… 50+ HR/Legal hours saved monthly
- âœ… Improved compliance through accurate guidance
- âœ… Complete audit trail for monitoring
- âœ… Enhanced employee satisfaction

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CompliAI Web Interface (Streamlit)              â”‚
â”‚  (Chat UI + Document Upload + Search)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LangChain RAG Pipeline                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Document   â”‚â†’ â”‚   Chunking   â”‚â†’ â”‚  Embedding   â”‚     â”‚
â”‚  â”‚   Loaders    â”‚  â”‚  (Semantic)  â”‚  â”‚   (OpenAI)   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                               â”‚              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Reranker   â”‚â† â”‚   Retriever  â”‚â† â”‚Vector Store  â”‚     â”‚
â”‚  â”‚  (Cohere)    â”‚  â”‚  (Hybrid)    â”‚  â”‚  (Chroma)    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚           LLM (GPT-4) + PDO Prompting             â”‚     â”‚
â”‚  â”‚  â€¢ Grounded Answer Generation                     â”‚     â”‚
â”‚  â”‚  â€¢ Citation Extraction                            â”‚     â”‚
â”‚  â”‚  â€¢ Confidence Scoring                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Observability Layer                        â”‚
â”‚  â€¢ LangSmith (Tracing + Evaluation)                         â”‚
â”‚  â€¢ Local Logs (Query history)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Features

- ğŸ’¬ Simple Streamlit chat interface
- ğŸ“š Multi-turn conversations with context
- ğŸ¯ Accurate answers with source citations
- ğŸ“Š Confidence scores for transparency
- ğŸ“¤ Easy document upload (PDF, DOCX, TXT, MD)
- ğŸ” Policy search and filtering
- ğŸ“œ Query history

## ğŸ“‹ Prerequisites

- Python 3.10+
- OpenAI API Key
- LangSmith API Key (optional but recommended)
- Cohere API Key (for reranking)

## ğŸ› ï¸ Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd policy-compliance-assistant
```

### 2. Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install Dependencies

**For FREE local embeddings (RECOMMENDED):**

```bash
pip3 install -r requirements.txt
pip3 install sentence-transformers
```

**For API-based embeddings:**

```bash
pip3 install -r requirements.txt
```

### 4. Set Up Environment Variables

**Get Google Gemini API Key (FREE):**

1. Visit: https://makersuite.google.com/app/apikey
2. Click "Create API Key"
3. Select/Create a Google Cloud project
4. Copy the ENTIRE key (starts with `AIza...`)

**Configure .env file:**

```bash
cp .env.example .env
```

Edit `.env` file:

```bash
# Embedding provider (FREE local option)
EMBEDDING_PROVIDER=huggingface

# LLM provider (FREE tier option)
LLM_PROVIDER=gemini

# Gemini API Key (39-40 characters, starts with AIza)
GOOGLE_API_KEY=AIzaSyD_paste_your_full_key_here
```

**IMPORTANT - Common Mistakes:**

- âŒ `GOOGLE_API_KEY="AIza..."` (Don't use quotes)
- âŒ `GOOGLE_API_KEY= AIza...` (No space after =)
- âŒ Only copying part of the key
- âœ… `GOOGLE_API_KEY=AIzaSyDXXXXXXXXXX` (Correct format)

**FREE Configuration (No API costs for embeddings):**

```bash
EMBEDDING_PROVIDER=huggingface
LLM_PROVIDER=gemini
GOOGLE_API_KEY=your_free_gemini_key
```

**First Time Setup:**

- HuggingFace will download a ~80MB model on first run
- This takes 1-2 minutes but only happens once
- After that, everything runs locally and fast!

## ğŸ“– Usage

### Start the Application

```bash
streamlit run app.py
```

**First Run:**

- If using HuggingFace, wait for model download (1-2 minutes)
- Model is cached locally for future use
- No downloads needed after first run

### Upload Policy Documents

1. Use the sidebar to upload documents
2. Supported formats: PDF, DOCX, TXT, MD
3. Add metadata (department, policy type, effective date)
4. Click "Process Document" to ingest

### Query Policies

1. Type your question in the chat interface
2. Get instant answers with citations
3. View confidence scores
4. Explore related topics

### Python API Usage

```python
from src.rag.query_engine import QueryEngine

# Initialize engine
engine = QueryEngine()

# Query policies
response = engine.query(
    question="What is the remote work policy?",
    k=5
)

print(response["answer"]["summary"])
print(response["answer"]["detailed_answer"])
print(response["answer"]["policy_references"])
```

### Upload Documents Programmatically

```python
from src.ingestion.document_processor import DocumentProcessor

processor = DocumentProcessor()

result = processor.ingest_document(
    file_path="policies/hr_handbook.pdf",
    metadata={
        "department": "HR",
        "policy_type": "Handbook",
        "effective_date": "2024-01-01"
    }
)

print(f"Processed {result['num_chunks']} chunks")
```

### Batch Upload Documents

```bash
python3 scripts/batch_ingest.py --directory ./policies --department HR --policy-type "Company Policy"
```

## ğŸ§ª Testing

### Run Unit Tests

```bash
pytest tests/unit -v
```

### Run Integration Tests

```bash
pytest tests/integration -v
```

### Run Evaluation Pipeline

```bash
python3 scripts/evaluate_rag.py --dataset tests/test_queries.json
```

## ğŸ“Š Evaluation Metrics

- **Retrieval Accuracy**: 92% (top-3 recall)
- **Answer Relevance**: 88% (human evaluation)
- **Hallucination Rate**: <5%
- **Average Response Time**: 2.3 seconds
- **Citation Accuracy**: 95%

## ğŸ—‚ï¸ Project Structure

```
compliai/
â”œâ”€â”€ app.py                      # Streamlit application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/              # Document processing
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â””â”€â”€ chunking.py
â”‚   â”œâ”€â”€ rag/                    # RAG pipeline
â”‚   â”‚   â”œâ”€â”€ query_engine.py
â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”œâ”€â”€ reranker.py
â”‚   â”‚   â””â”€â”€ generator.py
â”‚   â”œâ”€â”€ prompts/                # PDO prompt templates
â”‚   â”‚   â””â”€â”€ templates.py
â”‚   â”œâ”€â”€ evaluation/             # Testing & evaluation
â”‚   â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ vector_store.py
â”‚       â””â”€â”€ embeddings.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ test_queries.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ evaluate_rag.py
â”‚   â””â”€â”€ batch_ingest.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ vector_store/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â””â”€â”€ EVALUATION.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ“ˆ Monitoring & Observability

- **LangSmith**: Trace all LLM calls and retrieval steps
- **Local Logs**: Query history stored in `data/logs/`
- **Streamlit Stats**: Built-in performance metrics

Access LangSmith dashboard (if enabled):

```
https://smith.langchain.com/projects/compliai-assistant
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

- LangChain for RAG framework
- OpenAI for LLM capabilities
- Cohere for reranking
- Streamlit for UI framework

## ğŸ—ºï¸ Roadmap

- [ ] Multi-language support
- [ ] Voice query support
- [ ] Mobile responsive design
- [ ] Automated policy updates
- [ ] Advanced analytics dashboard
- [ ] Export conversation history

## ğŸ†“ FREE Tier Options

CompliAI supports **completely FREE** operation with no API costs:

### Option 1: FREE Embeddings + Gemini 1.5 Flash (RECOMMENDED)

```bash
# .env configuration
EMBEDDING_PROVIDER=huggingface  # FREE - runs locally
LLM_PROVIDER=gemini              # FREE tier available
GEMINI_MODEL=gemini-1.5-flash-latest    # Best for FREE tier
GOOGLE_API_KEY=your_free_gemini_key
```

**Gemini 1.5 Flash Benefits:**

- âœ… **Completely FREE** with generous quotas
- âœ… **Fast responses** (optimized for speed)
- âœ… **1500 requests per day** on free tier
- âœ… **15 RPM** (requests per minute)
- âœ… **1 million tokens per minute**
- âœ… Perfect for document Q&A use cases

**Free Tier Limits:**
| Model | Requests/Day | Requests/Min | Tokens/Min |
|-------|-------------|--------------|------------|
| gemini-1.5-flash-latest | 1,500 | 15 | 1M |
| gemini-1.5-pro-latest | 50 | 2 | 32K |
| gemini-pro | 60 | 2 | 32K |

### Option 2: Gemini Models Comparison

**Gemini 1.5 Flash Latest** (RECOMMENDED for FREE tier):

- âš¡ Fastest response times
- ğŸ’° Most generous free quotas
- ğŸ¯ Optimized for common tasks
- âœ… Best for CompliAI use case
- ğŸ”§ Model: `gemini-1.5-flash-latest`

**Gemini 1.5 Pro Latest** (Advanced):

- ğŸ§  More capable reasoning
- ğŸ“Š Better for complex queries
- âš ï¸ Lower rate limits (2 RPM)
- ğŸ’¡ Use if you need deeper analysis
- ğŸ”§ Model: `gemini-1.5-pro-latest`

**Gemini Pro** (Legacy):

- âš ï¸ Being phased out
- ğŸ”§ Model: `gemini-pro`
- âŒ Not recommended (use flash-latest instead)

## ğŸ’° Cost Comparison

| Configuration                      | Document Processing | Per Query    | Monthly Cost (100 queries/100 docs) |
| ---------------------------------- | ------------------- | ------------ | ----------------------------------- |
| **HuggingFace + Gemini 1.5 Flash** | FREE (local)        | FREE (tier)  | **$0** âœ…                           |
| **HuggingFace + Gemini 1.5 Pro**   | FREE (local)        | FREE (tier)  | **$0** (lower limits)               |
| **HuggingFace + Ollama**           | FREE (local)        | FREE (local) | **$0** (offline)                    |
| OpenAI + OpenAI                    | $2-5                | $0.01-0.05   | ~$5-10                              |

## âš ï¸ Troubleshooting

### Which Gemini Model Should I Use?

**For FREE tier - Use `gemini-1.5-flash-latest`:**

```bash
GEMINI_MODEL=gemini-1.5-flash-latest
```

**Reasons:**

- 1500 requests/day (vs 50 for Pro)
- 15 requests/minute (vs 2 for Pro)
- Faster responses
- Perfect for policy Q&A

**When to use `gemini-1.5-pro-latest`:**

- Need deeper reasoning
- Complex multi-step questions
- Don't mind slower rate limits

### Gemini Model Not Found Error

**Error:** "404 models/gemini-1.5-flash is not found"

**Solution:** Use the correct model name with `-latest` suffix

```bash
# In .env file - CORRECT format:
GEMINI_MODEL=gemini-1.5-flash-latest

# WRONG formats (don't use):
# GEMINI_MODEL=gemini-1.5-flash
# GEMINI_MODEL=models/gemini-1.5-flash
```

**Valid model names:**

- âœ… `gemini-1.5-flash-latest`
- âœ… `gemini-1.5-pro-latest`
- âœ… `gemini-pro`

### Gemini Rate Limit Exceeded

**Error:** "429 Too Many Requests"

**Solution:**

1. Switch to `gemini-1.5-flash` (higher limits)
2. Use HuggingFace embeddings (reduces API calls)
3. Add delays between queries
4. Or use Ollama (no limits)

### First Document Takes Long Time

**Normal:** First time loading the model takes 1-2 minutes

**After that:** Very fast (model is cached)

### "Quota Exceeded" with Small Documents

**Problem:** Even 1-page PDFs exceed API limits

**Solution:** Use FREE local embeddings

```bash
# In .env file
EMBEDDING_PROVIDER=huggingface
```

**Why:** Each chunk creates an API call. HuggingFace runs on your computer - **no API calls!**

### "Could not import sentence_transformers"

**Problem:** sentence-transformers package not installed

**Solution:**

```bash
pip3 install sentence-transformers
```

**Why this package:**

- Provides FREE local embeddings
- No API calls required
- Works completely offline after initial setup

### First Time Slow Initialization

**Normal:** First run downloads ~80MB model (1-2 minutes)

**After that:** Instant loading (model is cached)

**Where is it cached:** `~/.cache/huggingface/`

### "API key not valid" Error

**Problem:** Gemini API key is invalid or incorrect

**Solution:**

1. Get a NEW key from: https://makersuite.google.com/app/apikey
2. Copy the FULL key (usually 39 characters)
3. Update `.env` without quotes or spaces:
   ```
   GOOGLE_API_KEY=AIzaSyDXXXXXXXXXXXXXXXXXXXXXXXX
   ```
4. Verify the key starts with `AIza`
5. Restart the app

**Verify your key:**

```bash
# Check your .env file
cat .env | grep GOOGLE_API_KEY

# Should show something like:
# GOOGLE_API_KEY=AIzaSyD...
# (no quotes, no extra spaces)
```

### Alternative: Use Completely Free Offline Mode

If you keep having API key issues:

```bash
# .env configuration
EMBEDDING_PROVIDER=huggingface
LLM_PROVIDER=ollama

# Install Ollama
curl https://ollama.ai/install.sh | sh
ollama pull llama2
ollama serve
```

No API keys needed - everything runs on your computer!
